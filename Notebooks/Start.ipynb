{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb59342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from enum import IntEnum\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "class ActionType(IntEnum):\n",
    "    DISCRETE = 0\n",
    "    CONTINUOUS = 1     \n",
    "\n",
    "def get_gaes(rewards, dones, values, next_values, gamma, lamda, normalize):\n",
    "    deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "    deltas = np.stack(deltas)\n",
    "    gaes = copy.deepcopy(deltas)\n",
    "    for t in reversed(range(len(deltas) - 1)):\n",
    "        gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "    target = gaes + values\n",
    "    if normalize:\n",
    "        gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "    return gaes, target\n",
    "\n",
    "class PPO(Model):\n",
    "    def __init__ (self, input_dim, action_dim, action_type:ActionType, env:Env):\n",
    "        super(PPO, self).__init__()\n",
    "        self.learning_rate = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.lamda = 0.95\n",
    "        self.ppo_eps = 0.2\n",
    "        self.normalize = True\n",
    "        self.epoch = 3\n",
    "        self.rollout = 1024\n",
    "        self.batch_size = 512\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_type:ActionType = action_type\n",
    "        self.env:Env = env\n",
    "        self.create_models()\n",
    "        self.opt = optimizers.Adam(learning_rate=self.learning_rate, )\n",
    "    \n",
    "    def create_models(self):\n",
    "        self.base_model = tf.keras.Sequential([\n",
    "            tf.keras.Input(shape=self.input_dim),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu')\n",
    "        ])\n",
    "        self.value_model = tf.keras.layers.Dense(1, activation='linear')\n",
    "        if self.action_type == ActionType.DISCRETE:\n",
    "            self.action_model = tf.keras.layers.Dense(self.action_dim, activation='linear')\n",
    "    \n",
    "    def sample_action(self, observation_logits):\n",
    "        base_logits = self.base_model(observation_logits)\n",
    "        if self.action_type == ActionType.DISCRETE:\n",
    "            action_logits = self.action_model(base_logits)\n",
    "            # Sample from the categorical distribution, this method does softmax behind the scene\n",
    "            action = tf.random.categorical(action_logits, num_samples=1)\n",
    "            return tf.squeeze(action, axis=-1)\n",
    "\n",
    "    def call(self, observation_logits):\n",
    "        base_logits = self.base_model(observation_logits)\n",
    "        value_logits = self.value_model(base_logits)\n",
    "        if self.action_type == ActionType.DISCRETE:\n",
    "            action_logits = self.action_model(base_logits)\n",
    "            action_logits = tf.nn.softmax(action_logits)\n",
    "        \n",
    "        return action_logits, value_logits\n",
    "    \n",
    "    def update_models(self, observation_list, next_observation_list, reward_list, done_list, action_list):\n",
    "        observations = tf.convert_to_tensor(observation_list)\n",
    "        next_observations = tf.convert_to_tensor(next_observation_list)\n",
    "        rewards = tf.convert_to_tensor(reward_list)\n",
    "        done = tf.convert_to_tensor(done_list, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action_list)\n",
    "\n",
    "        old_policy, current_value = self(observations)\n",
    "        _, next_value = self(next_observations)\n",
    "        current_value = tf.squeeze(current_value, axis=-1)\n",
    "        next_value = tf.squeeze(next_value, axis=-1)\n",
    "\n",
    "        adv, target = get_gaes(\n",
    "            rewards=rewards,\n",
    "            dones=done,\n",
    "            values=current_value,\n",
    "            next_values=next_value,\n",
    "            gamma=self.gamma,\n",
    "            lamda=self.lamda,\n",
    "            normalize=self.normalize\n",
    "        )\n",
    "        # print(f\"Total run in this episode {len(observation_list)}\")\n",
    "        # print(self.trainable_variables)\n",
    "        for _ in range(self.epoch):\n",
    "            sample_range = np.arange(self.rollout)\n",
    "            np.random.shuffle(sample_range)\n",
    "            sample_idx = sample_range[:self.batch_size]\n",
    "\n",
    "            batch_state = [observations[i] for i in sample_idx]\n",
    "            batch_action = [actions[i] for i in sample_idx]\n",
    "            batch_target = [target[i] for i in sample_idx]\n",
    "            batch_adv = [adv[i] for i in sample_idx]\n",
    "            batch_old_policy = [old_policy[i] for i in sample_idx]\n",
    "\n",
    "            ppo_variable = self.trainable_variables\n",
    "            with tf.GradientTape() as tape:\n",
    "                train_policy, train_current_value = self(tf.convert_to_tensor(batch_state, dtype=tf.float32))\n",
    "                train_current_value = tf.squeeze(train_current_value, axis=-1)\n",
    "                train_adv = tf.convert_to_tensor(batch_adv, dtype=tf.float32)\n",
    "                train_target = tf.convert_to_tensor(batch_target, dtype=tf.float32)\n",
    "                train_action = tf.convert_to_tensor(batch_action, dtype=tf.int32)\n",
    "                train_old_policy = tf.convert_to_tensor(batch_old_policy, dtype=tf.float32)\n",
    "\n",
    "                entropy = tf.reduce_mean(-train_policy * tf.math.log(train_policy + 1e-8)) * 0.1\n",
    "                onehot_action = tf.one_hot(train_action, self.action_dim)\n",
    "                selected_prob = tf.reduce_sum(train_policy * onehot_action, axis=1)\n",
    "                selected_old_prob = tf.reduce_sum(train_old_policy * onehot_action, axis=1)\n",
    "                logpi = tf.math.log(selected_prob + 1e-8)\n",
    "                logoldpi = tf.math.log(selected_old_prob + 1e-8)\n",
    "\n",
    "                ratio = tf.exp(logpi - logoldpi)\n",
    "                clipped_ratio = tf.clip_by_value(ratio, clip_value_min=1-self.ppo_eps, clip_value_max=1+self.ppo_eps)\n",
    "                minimum = tf.minimum(tf.multiply(train_adv, clipped_ratio), tf.multiply(train_adv, ratio))\n",
    "                pi_loss = -tf.reduce_mean(minimum) + entropy\n",
    "\n",
    "                value_loss = tf.reduce_mean(tf.square(train_target - train_current_value))\n",
    "\n",
    "                total_loss = pi_loss + value_loss\n",
    "\n",
    "            grads = tape.gradient(total_loss, ppo_variable)\n",
    "            self.opt.apply_gradients(zip(grads, ppo_variable))        \n",
    "\n",
    "    \n",
    "    def learn(self, episodes=1000):\n",
    "        recent_rewards = []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            observation_list, next_observation_list = [], []\n",
    "            reward_list, done_list, action_list = [], [], []\n",
    "            total_timesteps = 0\n",
    "            episode_rewards = []\n",
    "\n",
    "            while total_timesteps < self.rollout:\n",
    "                obs, _ = self.env.reset()  # No fixed seed for diversity\n",
    "                done = False\n",
    "                ep_reward = 0\n",
    "\n",
    "                while not done:\n",
    "                    obs_tensor = tf.convert_to_tensor([obs], dtype=tf.float32)\n",
    "                    action = self.sample_action(obs_tensor)\n",
    "                    action = action.numpy()\n",
    "\n",
    "                    next_obs, reward, terminated, truncated, _ = self.env.step(action[0])\n",
    "                    done_flag = int(terminated or truncated)\n",
    "                    if terminated:\n",
    "                        reward = -2.0  # Pole fell\n",
    "                    elif truncated:\n",
    "                        reward = 2.0 \n",
    "                    # Store transition\n",
    "                    observation_list.append(obs)\n",
    "                    next_observation_list.append(next_obs)\n",
    "                    reward_list.append(reward)\n",
    "                    done_list.append(done_flag)\n",
    "                    action_list.append(action[0])\n",
    "\n",
    "                    obs = next_obs\n",
    "                    ep_reward += reward\n",
    "                    total_timesteps += 1\n",
    "\n",
    "                    if done_flag or total_timesteps >= self.rollout:\n",
    "                        break\n",
    "\n",
    "                episode_rewards.append(ep_reward)\n",
    "            # Update PPO model\n",
    "            self.update_models(observation_list, next_observation_list, reward_list, done_list, action_list)\n",
    "\n",
    "            # Log reward\n",
    "            avg_reward = np.mean(episode_rewards)\n",
    "            recent_rewards.append(avg_reward)\n",
    "            if len(recent_rewards) > 100:\n",
    "                recent_rewards.pop(0)\n",
    "            running_avg = np.mean(recent_rewards)\n",
    "\n",
    "            print(f\"Episode {episode} | Avg Reward: {avg_reward:.2f} | Running Avg: {running_avg:.2f}\")\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "ppo_trainer = PPO(env.observation_space.shape, env.action_space.n , 0, env)\n",
    "# ppo_trainer.base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71346f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51743c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
